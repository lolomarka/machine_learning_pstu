# Снижение размерности

Повышение точности (решения задачи) модели машинного обучения часто прямо пропорционально увеличению числа признаков, которыми она оперирует

Хорошим примером такого масштабирования являются современные языковые модели: модели на 2-3 миллиарда параметров существенно уступают моделям на 70 миллиардов и выше.

Но уже сам порядок чисел — миллиарды параметров — сообщает о том, что для работы, обучения и модификации подобных моделей нужны большие объемы вычислительных мощностей, оперативной памяти (и/или видеопамяти на GPU) и данных для тренировки и валидации.

И сейчас можно во многом поставить знак равенства между большой моделью и условно хорошей моделью — рекомендуется прочесть эссе [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), посвященное этой проблеме.

Что делать, если у нас нет ресурсов для больших моделей? Неужели нельзя добиваться результатов меньшими средствами?

Можно! Если рассмотреть большие модели, то можно выявить два момента:

1. Не все признаки вносят существенный вклад в повышение точности модели
2. Рост числа признаков всегда приводит к падению производительности

Следовательно, часто такие модели страдают от эффекта убывающей отдачи (diminishing returns)

Применительно к моделям машинного обучения такой эффект также называют «проклятием размерности» (curse of dimensionality):

1. Размерность набора данных является существенным препятствием эффективности моделей и алгоритмов
2. Повышение размерности не всегда ведет к повышению точности модели
3. Недостаток размерности ведет к недообучению модели
4. Избыток размерности ведет к переобучению модели

При росте числа измерений в $n$-мерном пространстве данные становятся разреженными: в них все больше «пустых» измерений. Одновременно с этим увеличиваются дистанции между точками в $n$-мерном пространстве, что затрудняет интерпретацию их взаимоположения.

Из «проклятия размерности» следует:

1. Проблемы оптимизации неизбежны с усложнением моделей и увеличением числа признаков
2. С увеличением размерности и, как следствие, разреженности данных все сложнее находить и идентифицировать определенные точки и их соседей

Для борьбы с «проклятием» как раз и применяются методы снижения размерности.

**Снижение размерности** — исключение из набора данных ряда признаков и создание ограниченного набора признаков, которые содержат всю необходимую информацию для работы модели (например, предсказания целевых величин) с большей точностью и эффективностью.

## Метод главных компонент (Principal Component Analysis, PCA)

Это линейный метод снижения размерности (в идеале снижение до 2D или 3D) для предсказательных (регрессионных) моделей. Метод определяет новую систему координат, в которой наибольшая дисперсия по любой проекции в исходном наборе данных лежит на первой оси; вторая по величине дисперсия — на второй оси и т.д.

Т.е. преобразование любого большого числа переменных в меньшее число некоррелированных переменных, которые и называются главными компонентами.

### Области применения метода главных компонент

1. Снижение размерности (снижение числа измерений в данных)
2. Выявление закономерности и паттернов в наборе данных высокой размерности
3. Визуализация данных высокой размерности
4. Фильтрация шума в данных
5. Улучшение классификации в задачах на классификацию

### Алгоритм

1. Нормализовать данные, например, через Z-score (Стандартизированная оценка)
2. Построить матрицу ковариации $N \times N$
3. Диагонализировать матрицу
4. Отсортировать значения векторов в матрице (от большего к меньшему)
5. Оставить только $K$ наибольших векторов
6. Модифицировать исходные данные
7. Вернуть набор данных меньшей размерности

![](https://s0.showslide.ru/s_slide/3236ebd0827562f74802a5de7005ab87/f1246198-0d98-42fe-8a71-8a736ab18ffc.jpeg)

![](https://helpiks.org/helpiksorg/baza1/48268912190.files/image307.jpg)

# Лабораторная работа №2. Реализация метода главных компонент

1. Загрузить из наборов данных Scikit-learn набор `breast cancer wisconsin dataset` (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) размерностью 30
2. Загрузить в отдельные переменные входные и выходные данные (`data`, `target`)
3. Реализовать метод главных компонент в функции, принимающей 2 аргумента: входные данные и искомое число главных компонент $K$ (рекомендуется визуализировать данные по мере возможности)
    1. Нормализовать данные, вычитая для каждого значения в колонке среднее значения этой колонки (`X.mean()`)
    2. Построить матрицу ковариации, используя метод NumPy `cov()`
    3. Диагонализировать матрицу методом NumPy `linalg.eig()`
    4. Отсортировать векторы, используя встроенный метод `argsort()`
    5. Используя синтаксис слайсинга, оставить $K$ главных компонент
    6. Используя метод `dot()`, модифицировать исходные нормализованные данные
    7. Вернуть данные сниженной размерности
4. Загрузить те же данные во встроенную в Scikit-learn реализацию метода главных компонент (`from sklearn.decomposition import PCA`)
5. Сравнить результаты вашей реализации и готовой
6. Найти оптимальное число главных компонент методом локтя:

![](https://upload.wikimedia.org/wikipedia/commons/e/e8/SCREE_plot.jpg)

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png)